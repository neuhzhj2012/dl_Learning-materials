## 基础知识

### 数学相关概念

###### 全概率公式

设事件B1、B2、B3…Bn 构成一个完备事件组，即它们两两互不相容，其和为全集；并且P(Bi)大于0，则对任一事件A发生的概率有如下表示。

$$
P(A)=\sum_{i=1}^{n}P(B_{i})P(A|B_{i})
$$

###### 贝叶斯公式

随机事件A发生的情况下，是由原因Bi引起的概率表示如下。

$$
P(B_{i}|A)=\frac{P(B_{i})P(A|B_{i})}{P(A)}=\frac{P(B_{i})P(A|B_{i})}{\sum_{i=1}^{n}P(B_{i})P(A|B_{i})}
$$

###### 先验概率

根据以往经验和分析得到的概率，**用于执因求果**。如全概率公式中P(Bi)为先验概率，用于求事件A发生的概率。

###### 后验概率

某件事已经发生，想要计算这件事发生的原因是由某个因素引起的概率，**用于知果求因**。如贝叶斯公式中P(Bi|A)为后验概率，用于表示A发生是由Bi事件引起的概率。

###### [ 似然函数](https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0)

是一种关于[统计模型](https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B "统计模型")中参数的函数，表示模型参数中的似然性。**似然性**则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值。可理解为已知事件A发生的条件下，该事物性质的参数是b的可能性。

$$
L(b|A) = \alpha P(A|B=b)
$$

似然函数的重要性不是它的具体取值，而是当参数变化时函数到底变小还是变大

对同一个似然函数，其所代表的模型中，某项参数值具有多种可能，但如果存在一个参数值，使得它的函数值达到最大的话，那么这个值就是该项参数最为“合理”的参数值。 即**极大(最大)似然估计**，表示**知果求最可能的原因**

对数函数是严格单增的，所以极大*对数似然*的解与极大似然的解是相同的

###### [先验概率、后验概率、似然函数及贝叶斯公式的理解](https://www.zhihu.com/question/24261751/answer/158547500)

$$
P(B_{i}|A)=\frac{P(B_{i})P(A|B_{i})}{P(A)}=\frac{P(B_{i})P(A|B_{i})}{\sum_{i=1}^{n}P(B_{i})P(A|B_{i})}
$$

贝叶斯公式中P(Bi|A)表示后验概率，P(Bi)表示先验概率，P(A|Bi)表示似然函数中参数是Bi时A的概率，P(A)表示一种事实的概率。

###### [拉格朗日函数](https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0)

在数学的最优化问题中，拉格朗日乘数法是一种寻找多元函数在其变量受到一个或多个条件约束时的极值的方法。

###### 线性方程

指未知数都是一次的方程，包含两个未知数的一次方程为二元一次方程。其本质是等式两边乘以任何相同的非零数，方程式的解不变。

###### 对偶形式

pass

### 数据结构相关概念

##### [树](https://zh.wikipedia.org/wiki/%E6%A0%91_%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84)

![树的深度与高度图解](D:\Project\MyGithub\dl_Learning-materials\material\img\树的高度与深度.jpg)

                                                            图片来着[这里 ](https://stackoverflow.com/questions/2603692/what-is-the-difference-between-tree-depth-and-height)          

- **深度**：对于任意节点n,n的深度为从根到n的唯一路径长，**根的深度为0**；
- **高度**：对于任意节点n,n的高度为从n到一片树叶的最长路径长，**所有树叶的高度为0**；
- **节点的度**：一个节点含有的子树的个数称为该节点的度；
- **树的度**：一棵树中，最大的节点度称为树的度；
- **叶节点**或**终端节点**：度为零的节点；
- **非终端节点**或**分支节点**：度不为零的节点；
- **父亲节点**或**父节点**：若一个节点含有子节点，则这个节点称为其子节点的父节点；
- **孩子节点**或**子节点**：一个节点含有的子树的根节点称为该节点的子节点；
- **兄弟节点**：具有相同父节点的节点互称为兄弟节点；
- 节点的**层次**：从根开始定义起，根为第1层，根的子节点为第2层，以此类推；
- **堂兄弟节点**：父节点在同一层的节点互为堂兄弟；
- **节点的祖先**：从根到该节点所经分支上的所有节点；
- **子孙**：以某节点为根的子树中任一节点都称为该节点的子孙。
- **森林**：由m（m>=0）棵互不相交的树的集合称为森林；

### 机器学习相关概念

###### 数据集

> 训练集用于训练模型；
> 
> 验证集用于模型的选择；
> 
> 测试集用于最终对学习方法的评估；

###### 统计学习方法概论

统计学习方法是基于数据构建概率统计模型并运用模型对数据进行预测与分析的学科，由**监督学习**（supervised learning），**非监督学习**，**半监督学习和强化学习**（reinforcement learning）等组成。统计学习方法的三要素如下。

> 模型：所要学习的条件概率分布或决策函数；
> 
> 策略：学习或选择最优模型的准则，即损失函数(度量模型一次预测的好坏)和风险函数(度量平均意义下模型预测的好坏)的构建；(定义损失函数，并将其极小化)
> 
> 算法：学习模型的具体计算方法，即求解最优化问题的算法

###### 生成模型与判别模型

- 定义

> 生成模型：由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型。即模型表示了给定输入X产生输出Y的生成关系；
> 
> 判别模型：由数据直接学习决策函数f(X)或条件概率分布P(Y|X)作为预测模型。即模型表示给定输入X时预测什么样的输出Y；

- 常见模型：

> 生成模型包括朴素贝叶斯法和隐马尔科夫模型；
> 
> 判别模型包括：感知机、k近邻法、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等；

- 优缺点

> 生成方法可还原出联合概率分布P(X,Y);学习收敛速度较判别方法更快；存在隐变量时仍可用生成方法，此时判别方法不可用；
> 
> 判别方法学习的准确率更高；可对数据进行抽象、定义特征并使用特征，简化学习问题。

##### 模型选择

旨在避免过拟合并提高模型的预测能力。**模型选择常用的方法：正则化和交叉验证**。模型对未知数据的预测能力称为**泛化能力**。

> 过拟合 指学习时选择的模型包含的参数过多，以致于出现这一模型对已知数据预测的很好，但对未知数据预测很差的现象

###### 正则化

正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。正则化项一般是模型复杂度的单调递增函数，如正则化项可以是模型参数向量的范数。

###### 交叉验证

数据充足时，选择对验证集有最小预测误差的模型。否则采用交叉验证的方法，其基本思想是重复的使用数据，把数据分为训练集与测试集，在此基础上反复训练、测试以及模型选择。包括简单交叉验证，S折交叉验证和留一交叉验证。

> 简单交叉验证 将数据切分为训练集(70%)和测试集(30%)，然后用训练集在不同条件下(eg:不同参数个数)训练模型；然后在测试集上评测各模型的测试误差，选择误差最小的模型；
> 
> S折交叉验证 随机将数据切分为S个互不相交大小相同的子集，然后利用S-1个子集作为训练集，剩余的一个作为测试集训练模型。将上述过程对可能的S中集合重复进行，最后选择S次评测中测试误差最小的模型。该方法应用最多；
> 
> 留一交叉验证 S折交叉验证的特殊情况，即S等于样本容量。

###### 泛化能力

通常基于测试误差评价模型的泛化能力，但由于测试集的有限性，导致评测结果是不可靠的。因此使用泛化误差来反应模型的泛化能力。

### 评价指标

##### 分类问题

pass

### 距离度量函数

##### Lp距离

向量xi与xj的Lp距离定义如下，其中p>=1。当p=2时称为**欧式距离**(Euclidean distance)，当p=1时为**曼哈顿距离**(Manhattan distance)

$$
L_{p}(x_{i}, x_{j})=\left ( \sum_{l=1}^{n}\left | x_{i}^{(l)} - x_{j}^{(l)} \right |^{p} \right )^{\frac{1}{p}}
$$

##### 函数汇总

- 符号函数

$$
sign(x)=\left\{\begin{matrix}
+1,  x\geq 0 & \\ 
-1,  x< 0& 
\end{matrix}\right.
$$

- 指示函数，表示其中有哪些元素属于某一子集*A*。

$$
I_{A}(x)=\left\{\begin{matrix}
1 & \\ 
0 & 
\end{matrix}\right.  \begin{matrix}
x \in A& \\ 
x \not\in A& 
\end{matrix}
$$

## 机器学习方法

##### 感知机

###### 原始形式

- 二分类的线性分类模型，输入为实例的特征向量，输出为实例的类别。**感知机模型为激活函数是符号函数的神经元**，是神经网络与支持向量机的基础。

$$
f(x)=sign(w\cdot x +b)
$$

- 学习策略为经验风险函数极小化，用于表示所有误分类点(集合M)到分类超平面的距离和(不考虑超平面的模值1/|w|)。

$$
L(w,b)=-\sum_{x_{i}\varepsilon M}y_{i}(w\cdot x_{i}+b)
$$

算法是误分类数据驱动的，采用随机梯度下降法(stochastic gradient descent)。首先，任选一个超平面w0,b0，然后用梯度下降法不断极小化目标函数。极小化过程中每次随机选取一个误分类点使其梯度下降。

> 直观理解是当一个实例点被误分类时，调整w,b的值，使分离超平面向该误分类点的一侧移动，以减少误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。

###### 对偶形式

原始形式的模型中计算量主要集中在w与x的內积，尤其当x的维度较高时，计算所有样本的分类结果将会变的很慢。通过损失函数计算变量w和b的梯度如下。

$$
\frac{\partial L(w,b)}{\partial w} = - \sum_{x_{i}\in M}y_{i}x_{i}
$$

$$
\frac{\partial L(w,b)}{\partial b} = - \sum_{x_{i}\in M}y_{i}
$$

所以基于随机梯度下降法进行更新时，每次选取一个误分类点(xi, yi)，对w，b更新后的结果如下。

$$
w\leftarrow w+\eta y_{i}x_{i}, b\leftarrow b+\eta y_{i}
$$

若设w,b的初始值均为0，则对上式逐步修改n次后，w,b关于(xi, yi)的增量分别为aiyixi和aiyi，故最终学到的w和b分别表示如下。

$$
w=\sum_{i=1}^{N}\alpha _{i}y_{i}x_{i}, b=\sum_{i=1}^{N}\alpha _{i}y_{i}, \alpha _{i}=n_{i}\eta 
$$

得到**感知机模型的对偶形式**如下。

$$
f(x)=sign(\sum_{j=1}^{N}\alpha _{j}y_{j}x_{j}\cdot x + b)
$$

**对偶形式中训练实例以內积的形式出现，所以可将训练集中的实例间的內积计算并存储在矩阵(Gram)中，这样通过查矩阵即可快速计算出当前样本分类的结果，减少了计算的时间**。其理解过程详见知乎文章：[如何理解感知机学习算法的对偶形式？](https://www.zhihu.com/question/26526858/answer/253579695)

##### K近邻法(k-nearest neighbor, k-NN)

###### 模型及算法

- 概览

是一种基本分类和回归方法，输入为实例的特征向量(特征空间的点)，输出为实例的类别。不具有显示的学习过程，其**直观理解**：给定一个训练数据集，对新的输入实例，在训练集中找到与该实例最近邻的k个实例，这k个实例的多数属于某个类，则该输入实例分为这个类。从中总结出该算法的**三个基本要素为：k值的选择、距离度量和分类决策规则**。

利用训练数据集对特征向量空间进行划分，可看做是训练集中的样本点，将特征空间中距离该点比其他样本点更近的所有点组成一个区域，称之为单元，这样不同单元的类别是确定的，可作为k近邻分类的模型；特征空间中两实例点的距离是其相似程度的反应，可以使用欧式距离，或更一般的Lp距离或Minkowski距离；分类决策规则为多数表决规则。

- 分类决策规则：多数表决

对多数表决规则的理解，假设分类损失为0-1损失函数，则误分类的概率如下。

$$
P(Y\neq f(x)))=1-P(y=f(x))
$$

那么对于训练集中的误分类率，可用如下公式表示。所以误分类率最小等价于将x所属邻域N中最多数的类别c作为x的类别y，即**多数表决规则等价于经验风险最小化**。eg:x的邻域共5个点(N)，2个类别为0，3个类别为1，则<u>区域N的类别c为1</u>，此时y=1时和区域N的类别相同，故可以减少0-1损失函数。

$$
\frac{1}{k}\sum_{x_{i}\in N_{k}(x)}I(y_{i}\neq c_{j})=1-\frac{1}{k}\sum_{x_{i}\in N_{k}(x)}I(y_{i}= c_{j})
$$

###### k近邻法的实现：kd树

- 由来

使用该算法进行预测时，需要快速完成距离计算和k近邻的搜索，最简单的方法是使用线性扫描和距离排序，计算输入实例与每个训练实例的距离并排序，此时若训练集很大，则会很耗时。kd树则用来减少计算 距离的次数，并提高k近邻搜索的效率。

- kd树的构建

**kd树**是对k维空间中的实例点进行存储以便对其快速检索的二叉树。构建过程相当于不断地用垂直于坐标轴的超平面将k维空间进行划分。具体的：以所有训练集中第一维特征的中位数为切分点，利用垂直于第一维坐标的直线将特征空间分为左右两部分，并将该切分点作为*树的根结点*；对深度为j的结点，选择第L维作为切分的坐标轴，其中L=j(modk) + 1，所有训练集中的第L维特征的中位数作为切分点，切分通过切分点并与L维坐标轴垂直的子特征空间，*切分点作为kd树当前的结点*；重复上述过程至两个子区域中没有训练集实例为止。

- kd树的最近邻搜索
1. 在kd树中找到包含目标点x的叶结点：从根结点出发，递归的访问kd树。若x的当前维坐标小于切分点坐标，则移动到左子结点，否则移动到右子结点，直到kd树的叶结点为止

2. 以此叶结点为“当前最近点”

3. 递归的向上回退，在每个结点进行一下操作：a)若当前结点距离目标点x的距离更近，则将其更新为“当前最近点”；b)检查"当前最近点"的兄弟结点对应的区域中是否有更近的点(以目标点为球心，目标点到当前最近点为半径，观察兄弟结点对应的区域中是否有实例点落在该球体内，若有则将其更新为“当前最近点“，并递归的进行最近邻搜索，若无则向上回退)

4. 当回退至根结点时，搜索结束。此时的”当前最近点“即为目标点x的最近邻点。




